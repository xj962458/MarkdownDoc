## 一、混淆矩阵

​		混淆矩阵是数据科学和机器学习中经常使用的用来总结分类模型预测结果的表，用n行n列的矩阵来表示，将数据集中的记录按照真实的类别和预测的类别两个标准进行汇总。以二分类任务为例，混淆矩阵的结构如下：

![20200904162935619](http://doc.xjfyt.top/markdown_img/20220920181733.png)

​		如上图所示，要了解各个评价指标，首先需要知道混淆矩阵，混淆矩阵中的P表示Positive，即正例或者阳性，N表示Negative，即负例或者阴性。表中FP表示实际为负但被预测为正的样本数量，TN表示实际为负被预测为负的样本的数量，TP表示实际为正被预测为正的样本数量，FN表示实际为正但被预测为负的样本的数量。另外，TP+FP=P’表示所有被预测为正的样本数量，同理FN+TN为所有被预测为负的样本数量，TP+FN为实际为正的样本数量，FP+TN为实际为负的样本数量。



## 二、准确率

准确率是分类正确的样本占总样本个数的比例，即
$$
Accuracy=\frac{n_{correct}}{n_{total}}
$$
其中，$n_{correct}$为被正确分类的样本个数，$n_{total}$ 为总样本个数。
结合上面的混淆矩阵，公式还可以这样写：
$$
Accuracy=\frac{TP+TN}{TP+TN+FP+FN}
$$

准确率是分类问题中最简单直观的评价指标，但存在明显的缺陷。比如如果样本中有99%的样本为正样本，那么分类器只需要一直预测为正，就可以得到99%的准确率，但其实际性能是非常低下的。也就是说，当不同类别样本的比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。

```python
import numpy as np
from sklearn.metrics import accuracy_score

y_pred = [0, 2, 1, 3]
y_true = [0, 1, 2, 3]
print(accuracy_score(y_true, y_pred))  # 0.5
print(accuracy_score(y_true, y_pred, normalize=False))  # 2

# 在具有二元标签指示符的多标签分类案例中
print(accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2))))  # 0.5
```



## 三、错误率



## 四、精确率（查准率）

精确率指模型预测为正的样本中实际也为正的样本占被预测为正的样本的比例。计算公式为：
$$
Precision=\frac{TP}{TP+FP}
$$



```python
from sklearn.metrics import precision_score

y_true = [0, 1, 2, 0, 1, 2]
y_pred = [0, 2, 1, 0, 0, 1]
print(precision_score(y_true, y_pred, average='macro'))  # 0.2222222222222222
print(precision_score(y_true, y_pred, average='micro'))  # 0.3333333333333333
print(precision_score(y_true, y_pred, average='weighted'))  # 0.2222222222222222
print(precision_score(y_true, y_pred, average=None))  # [0.66666667 0.0.]
```



## 五、召回率（查全率）

$$
Recall=\frac{TP}{TP+FN}
$$

召回率指实际为正的样本中被预测为正的样本所占实际为正的样本的比例

```bash
from sklearn.metrics import recall_score

y_true = [0, 1, 2, 0, 1, 2]
y_pred = [0, 2, 1, 0, 0, 1]
print(recall_score(y_true, y_pred, average='macro'))  # 0.3333333333333333
print(recall_score(y_true, y_pred, average='micro'))  # 0.3333333333333333
print(recall_score(y_true, y_pred, average='weighted'))  # 0.3333333333333333
print(recall_score(y_true, y_pred, average=None))  # [1. 0. 0.]
```



## 六、F1 score

​       查准率和查全率是一对矛盾的度量。一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。通常只有在一些简单任务中，才可能使二者都很高。F1 Score是Precision和Recall加权调和平均数，并假设两者一样重要。

​		Precision体现了模型对负样本的区分能力，Precision越高，模型对负样本的区分能力越强；Recall体现了模型对正样本的识别能力，Recall越高，模型对正样本的识别能力越强。F1 score是两者的综合，F1 score越高，说明模型越稳健。
$$
F1=\frac{2*precision*recall}{precision+recall}
$$

```python
from sklearn.metrics import f1_score

y_true = [0, 1, 2, 0, 1, 2]
y_pred = [0, 2, 1, 0, 0, 1]
print(f1_score(y_true, y_pred, average='macro'))  # 0.26666666666666666
print(f1_score(y_true, y_pred, average='micro'))  # 0.3333333333333333
print(f1_score(y_true, y_pred, average='weighted'))  # 0.26666666666666666
print(f1_score(y_true, y_pred, average=None))  # [0.8 0.  0. ]
```



## 七、P-R曲线

评价一个模型的好坏，不能仅靠精确率或者召回率，最好构建多组精确率和召回率，绘制出模型的P-R曲线。
下面说一下P-R曲线的绘制方法。P-R曲线的横轴是召回率，纵轴是精确率。P-R曲线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本，小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率。整条P-R曲线是通过将阈值从高到低移动而生成的。原点附近代表当阈值最大时模型的精确率和召回率。

![img](http://doc.xjfyt.top/markdown_img/20220921205842.jpeg)

## 八、ROC曲线

**ROC（Receiver Operating Characteristic）曲线，又称接受者操作特征曲线。**曲线对应的纵坐标是TPR，横坐标是FPR。

![img](http://doc.xjfyt.top/markdown_img/20220921210102.jpeg)

## 九、AUC面积

AUC 是 Area Under Curve 的简称，顾名思义，它表示的是“曲线下的面积”。这里的“曲线”就是我们前面提到的 ROC 曲线。AUC 就是 ROC 曲线下的面积总和，该值能够量化反映分类算法的性能。
计算 AUC 的值并不复杂，只需要沿着 ROC 曲线的横轴做积分（或累加求和）即可。通常，ROC 曲线都位于 y=x 这条线的上方（如果不是这样的，只需要把模型预测概率 P 反转成 1-P 能得到一个更好的分类器）。
因此，AUC 的取值范围一般是 0.5～1。通常来说，AUC 越大表明分类器性能越好，因为它可以把真正的正类样本排在前面，降低误判率。
————————————————
版权声明：本文为CSDN博主「想进步的小孟」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_37701945/article/details/122589097
